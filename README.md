# LLM Compression

> **⚠️ AI-Generated Content Warning**
> 
> This repository, including this README and virtually all of its content, was generated by Imbue Sculptor (an AI coding agent). While efforts have been made to ensure accuracy and functionality, there may be mistakes, hallucinations, or inconsistencies in the code, documentation, or explanations provided.
> 
> Please review all code carefully before use and verify any claims or implementations independently.


An experimental exploration of using Large Language Models (LLMs) for text compression.

## Motivation

This project explores a novel approach to text compression inspired by mobile phone autocomplete behavior. The core insight is that if you repeatedly select the most likely next token predicted by an LLM, you generate text that contains very little "information" and should be highly compressible.

The key idea: instead of storing raw text, we can encode text as a sequence of **token ranks** - where each token is represented by its likelihood rank in the LLM's prediction distribution. This rank-based encoding should be much more compressible than raw text using traditional compression algorithms.

## How It Works

The compression pipeline consists of two main phases:

### Phase 1: LLM Rank Encoding
- Tokenize the input text
- For each token, determine its rank in the LLM's probability distribution given the preceding context
- Store the sequence of ranks instead of raw tokens

### Phase 2: Traditional Compression
- Apply conventional compression algorithms (zlib, Huffman coding, etc.) to the rank sequence
- The rank sequence should exhibit patterns that make it highly compressible

## Implementations

This repository contains two parallel implementations:

- **[Python Implementation](python/)** - Original research implementation with comprehensive features
- **[JavaScript Implementation](javascript/)** - Standalone browser-compatible version using transformers.js

Both implementations provide:
- LLM-based token ranking using GPT-2 and other models
- Multiple compression algorithms (gzip, Huffman coding, custom methods)
- Round-trip verification to ensure perfect reconstruction
- Interactive web interfaces for visualization
- Comprehensive benchmarking and analysis tools

## Key Features

- **Multiple LLM Models**: Support for GPT-2, GPT-3.5, and other transformer models
- **Configurable Context Length**: Adjustable context window for efficiency vs. accuracy trade-offs
- **Lossless Compression**: Perfect reconstruction of original text from compressed ranks
- **Visualization Tools**: Interactive web interfaces to understand token ranking behavior
- **Comprehensive Testing**: Full test suites ensuring correctness across both implementations

## Experimental Variables

This exploration investigates several dimensions:

### LLM Rank Encoding Variables
- **Model Choice**: Different model architectures and sizes
- **Context Length**: Impact of context window size on compression ratio
- **Batch Processing**: Techniques to speed up the ranking process
- **Model Optimization**: Trade-offs between accuracy and speed

### Compression Algorithm Variables
- **Traditional Methods**: zlib, Huffman coding, arithmetic coding
- **Distribution Modeling**: Parametric distributions (Zipfian, power law)
- **Adaptive Algorithms**: Custom methods optimized for rank distributions
- **Hierarchical Approaches**: Different compression strategies for different text types

## Getting Started

Choose your preferred implementation:

- **Python**: See [python/README.md](python/README.md) for installation and usage
- **JavaScript**: See [javascript/README.md](javascript/README.md) for browser-based usage

Both implementations are fully functional and provide comparable results.

## Research Applications

This work has potential applications in:
- **Text Compression**: Novel approach to lossless text compression
- **Language Model Analysis**: Understanding predictability patterns in different text types
- **Information Theory**: Measuring "surprise" and information content in natural language
- **Compression Benchmarking**: Comparing compression efficiency across different domains

## Contributing

This is an experimental research project. Contributions, ideas, and discussions are welcome!

## License

MIT License - see individual implementation directories for details.
